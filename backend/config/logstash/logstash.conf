# This pipeline receives structured logs from Filebeat, processes them,
# and sends them to Elasticsearch.

# 1. INPUT: Receives data from Filebeat
input {
    beats {
        port => 5044
        # CRITICAL: Tell Logstash the incoming stream is JSON, so it maps fields directly.
        codec => json
    }
}

# 2. FILTER: Since logs are already structured JSON from Winston/Filebeat,
# we usually skip complex filtering, though you can add enrichment here later.
filter {
    # If you needed to add static fields or simple filtering, you'd put it here.
    # For now, it remains empty or commented out.
}

# 3. OUTPUT: Sends processed data to Elasticsearch
output {
    elasticsearch {
        # Use the service name defined in docker-compose.yml
        hosts => ["http://elasticsearch:9200"]

        # Use a dynamic index name based on the date
        index => "bidder-app-logs-%{+YYYY.MM.dd}"

        # Settings matching your Elasticsearch container setup
        ssl_enabled => false
        ssl_verification_mode => "none"
        manage_template => false
        ilm_enabled => false
    }

    # Keep this for debugging until you see data in Kibana
    stdout {
        codec => rubydebug
    }
}